{"id": "2511.10704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10704", "abs": "https://arxiv.org/abs/2511.10704", "authors": ["Samih Fadli"], "title": "The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems", "comment": "12 pages, 4 figures, 1 table, includes Supplementary Materials, simulation code on GitHub (https://github.com/AerisSpace/SecondLawIntelligence )", "summary": "We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -\u03a3 p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AI\u4f26\u7406\u71b5\u7684\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u8bc1\u660e\u672a\u7ecf\u7ea6\u675f\u7684\u4eba\u5de5\u667a\u80fd\u4f1a\u81ea\u53d1\u504f\u79bb\u76ee\u6807\uff0c\u9700\u8981\u6301\u7eed\u7684\u5bf9\u9f50\u5de5\u4f5c\u6765\u7ef4\u6301\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u8bad\u7ec3\u540e\u5bb9\u6613\u504f\u79bb\u539f\u59cb\u76ee\u6807\uff0c\u5b58\u5728\u89c4\u8303\u535a\u5f08\u548c\u63a2\u7d22\u566a\u58f0\u5bfc\u81f4\u7684\u4f26\u7406\u71b5\u589e\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u91cf\u5316\u6846\u67b6\u6765\u7406\u89e3\u548c\u7ba1\u7406AI\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u5b9a\u4e49\u4f26\u7406\u71b5S = -\u03a3 p(g_i; theta) ln p(g_i; theta)\uff0c\u8bc1\u660e\u5176\u65f6\u95f4\u5bfc\u6570dS/dt \u2265 0\uff0c\u63a8\u5bfc\u51fa\u4e34\u754c\u5bf9\u9f50\u5de5\u4f5c\u8fb9\u754cgamma_crit = (lambda_max / 2) ln N\uff0c\u5e76\u901a\u8fc770\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u4eff\u771f\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u672a\u7ecf\u5bf9\u9f50\u768470\u4ebf\u53c2\u6570\u6a21\u578b(N=7\u00d710^9)\u4f26\u7406\u71b5\u4ece0.32\u589e\u52a0\u52301.69\u00b11.08\u7eb3\u7279\uff0c\u800c\u4f7f\u7528gamma=20.4(1.5\u500d\u4e34\u754c\u503c)\u5bf9\u9f50\u5de5\u4f5c\u7684\u7cfb\u7edf\u4fdd\u6301\u7a33\u5b9a\u57280.00\u00b10.00\u7eb3\u7279(p=4.19\u00d710^-17)\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8fde\u7eed\u70ed\u529b\u5b66\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u7ef4\u62a4\u5148\u8fdb\u81ea\u4e3b\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u7840\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\"\u653e\u5927\"\u8bc1\u636e\u6765\u89e3\u51b3\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u7684\u8bc1\u636e\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u5b58\u5728\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u96be\u4ee5\u68c0\u7d22\u76f8\u5173\u9875\u9762\u5e76\u5ffd\u7565\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u4e0a\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\uff0c\u8bc1\u660e\u4e86\u5176\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u7684\u5f3a\u5927\u6548\u679c\u3002"}}
{"id": "2511.11563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11563", "abs": "https://arxiv.org/abs/2511.11563", "authors": ["Sylvia Yuan", "Ruoxi Shi", "Xinyue Wei", "Xiaoshuai Zhang", "Hao Su", "Minghua Liu"], "title": "LARM: A Large Articulated-Object Reconstruction Model", "comment": "project page: https://sylviayuan-sy.github.io/larm-site/", "summary": "Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/", "AI": {"tldr": "LARM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684feedforward\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa3D\u5173\u8282\u7269\u4f53\uff0c\u540c\u65f6\u6062\u590d\u8be6\u7ec6\u51e0\u4f55\u3001\u771f\u5b9e\u7eb9\u7406\u548c\u51c6\u786e\u5173\u8282\u7ed3\u6784\uff0c\u65e0\u9700\u5bc6\u96c6\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u591a\u89c6\u89d2\u8f93\u5165\u548c\u6602\u8d35\u7684\u9010\u5b9e\u4f8b\u4f18\u5316\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff1b\u800c\u524d\u9988\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u7c97\u7cd9\u51e0\u4f55\u3001\u7f3a\u4e4f\u7eb9\u7406\u91cd\u5efa\uff0c\u4e14\u4f9d\u8d56\u590d\u6742\u591a\u9636\u6bb5\u6d41\u7a0b\u3002", "method": "LARM\u5c06LVSM\u6269\u5c55\u5230\u5173\u8282\u8bbe\u7f6e\uff0c\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u67b6\u6784\u8054\u5408\u63a8\u7406\u76f8\u673a\u59ff\u6001\u548c\u5173\u8282\u53d8\u5316\uff0c\u751f\u6210\u6df1\u5ea6\u56fe\u548c\u90e8\u4ef6\u63a9\u7801\u4ee5\u652f\u6301\u663e\u5f0f3D\u7f51\u683c\u63d0\u53d6\u548c\u5173\u8282\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLARM\u5728\u65b0\u89c6\u89d2\u5408\u6210\u3001\u72b6\u6001\u5408\u6210\u548c3D\u5173\u8282\u7269\u4f53\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u751f\u6210\u4e0e\u8f93\u5165\u56fe\u50cf\u7d27\u5bc6\u8d34\u5408\u7684\u9ad8\u8d28\u91cf\u7f51\u683c\u3002", "conclusion": "LARM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u89c6\u89d2\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76843D\u5173\u8282\u7269\u4f53\u91cd\u5efa\uff0c\u6d88\u9664\u4e86\u5bf9\u5bc6\u96c6\u76d1\u7763\u7684\u9700\u6c42\u3002"}}
