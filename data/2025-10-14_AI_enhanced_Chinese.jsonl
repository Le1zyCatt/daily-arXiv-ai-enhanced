{"id": "2510.09011", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09011", "abs": "https://arxiv.org/abs/2510.09011", "authors": ["Yincen Qu", "Huan Xiao", "Feng Li", "Hui Zhou", "Xiangying Dai"], "title": "TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation", "comment": null, "summary": "Travel planning is a valuable yet complex task that poses significant\nchallenges even for advanced large language models (LLMs). While recent\nbenchmarks have advanced in evaluating LLMs' planning capabilities, they often\nfall short in evaluating feasibility, reliability, and engagement of travel\nplans. We introduce a comprehensive benchmark for travel planning that unifies\nfine-grained criteria into a single reward, enabling direct comparison of plan\nquality and seamless integration with reinforcement learning (RL). Our\nevaluator achieves moderate agreement with travel-expert annotations (60.75\\%)\nand outperforms multiple LLM-as-judge baselines. We further release a\nlarge-scale dataset of 4,870 queries including 219 real-world, free-form\nrequests for generalization to authentic user intent. Using this benchmark, we\nconduct extensive experiments across diverse methods and LLMs, including\ntest-time computation, neuro-symbolic approaches, supervised fine-tuning, and\nRL via GRPO. Across base models, RL generally improves itinerary feasibility\nover prompt-only and supervised baselines, yielding higher unified reward\nscores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u65c5\u884c\u89c4\u5212\u57fa\u51c6\uff0c\u901a\u8fc7\u5355\u4e00\u5956\u52b1\u6574\u5408\u7ec6\u7c92\u5ea6\u6807\u51c6\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86RL\u65b9\u6cd5\u5728\u53ef\u884c\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u8bc4\u4f30\u65c5\u884c\u8ba1\u5212\u7684\u53ef\u884c\u6027\u3001\u53ef\u9760\u6027\u548c\u5438\u5f15\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540cLLM\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b4870\u4e2a\u67e5\u8be2\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u7edf\u4e00\u7684\u5956\u52b1\u8bc4\u4f30\u5668\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u3001\u76d1\u7763\u5fae\u8c03\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u79cd\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u5668\u4e0e\u65c5\u6e38\u4e13\u5bb6\u6807\u6ce8\u8fbe\u523060.75%\u7684\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u591a\u4e2aLLM\u4f5c\u4e3a\u8bc4\u5224\u57fa\u51c6\u3002RL\u65b9\u6cd5\u5728\u884c\u7a0b\u53ef\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u4ec5\u63d0\u793a\u548c\u76d1\u7763\u57fa\u7ebf\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u7edf\u4e00\u5956\u52b1\u5206\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u65c5\u884c\u89c4\u5212\u8d28\u91cf\uff0cRL\u65b9\u6cd5\u5728\u63d0\u9ad8\u89c4\u5212\u53ef\u884c\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3aLLM\u7684\u65c5\u884c\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.08791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08791", "abs": "https://arxiv.org/abs/2510.08791", "authors": ["Yuanhao Zou", "Zhaozheng Yin"], "title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering", "comment": "CVPR2025 Paper", "summary": "Medical Visual Question Answering (Med-VQA) is a challenging task that\nrequires a deep understanding of both medical images and textual questions.\nAlthough recent works leveraging Medical Vision-Language Pre-training (Med-VLP)\nhave shown strong performance on the Med-VQA task, there is still no unified\nsolution for modality alignment, and the issue of hard negatives remains\nunder-explored. Additionally, commonly used knowledge fusion techniques for\nMed-VQA may introduce irrelevant information. In this work, we propose a\nframework to address these challenges through three key contributions: (1) a\nunified solution for heterogeneous modality alignments across multiple levels,\nmodalities, views, and stages, leveraging methods like contrastive learning and\noptimal transport theory; (2) a hard negative mining method that employs soft\nlabels for multi-modality alignments and enforces the hard negative pair\ndiscrimination; and (3) a Gated Cross-Attention Module for Med-VQA that\nintegrates the answer vocabulary as prior knowledge and selects relevant\ninformation from it. Our framework outperforms the previous state-of-the-art on\nwidely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6311\u6218\u7684\u4e09\u91cd\u8d21\u732e\u6846\u67b6\uff1a\u7edf\u4e00\u7684\u591a\u5c42\u6b21\u6a21\u6001\u5bf9\u9f50\u65b9\u6848\u3001\u4f7f\u7528\u8f6f\u6807\u7b7e\u7684\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\u65b9\u6cd5\uff0c\u4ee5\u53ca\u96c6\u6210\u7b54\u6848\u8bcd\u6c47\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u7684\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u3002", "motivation": "\u5f53\u524dMed-VQA\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6a21\u6001\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u56f0\u96be\u8d1f\u6837\u672c\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u5e38\u7528\u77e5\u8bc6\u878d\u5408\u6280\u672f\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u4fe1\u606f\u3002", "method": "1) \u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u5b9e\u73b0\u591a\u5c42\u6b21\u3001\u591a\u6a21\u6001\u3001\u591a\u89c6\u56fe\u548c\u591a\u9636\u6bb5\u7684\u7edf\u4e00\u6a21\u6001\u5bf9\u9f50\uff1b2) \u91c7\u7528\u8f6f\u6807\u7b7e\u8fdb\u884c\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\u5e76\u52a0\u5f3a\u56f0\u96be\u8d1f\u6837\u672c\u5bf9\u533a\u5206\uff1b3) \u8bbe\u8ba1\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u96c6\u6210\u7b54\u6848\u8bcd\u6c47\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728RAD-VQA\u3001SLAKE\u3001PathVQA\u548cVQA-2019\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684Med-VQA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u6a21\u6001\u5bf9\u9f50\u3001\u6709\u6548\u7684\u56f0\u96be\u8d1f\u6837\u672c\u5904\u7406\u548c\u7cbe\u786e\u7684\u77e5\u8bc6\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u7684\u6027\u80fd\u3002"}}
