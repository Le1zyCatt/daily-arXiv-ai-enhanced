<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2510.08791)
*Yuanhao Zou,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 提出一个解决医学视觉问答挑战的三重贡献框架：统一的多层次模态对齐方案、使用软标签的困难负样本挖掘方法，以及集成答案词汇作为先验知识的门控交叉注意力模块。


<details>
  <summary>Details</summary>
Motivation: 当前Med-VQA任务中缺乏统一的模态对齐解决方案，困难负样本问题研究不足，且常用知识融合技术可能引入无关信息。

Method: 1) 使用对比学习和最优传输理论实现多层次、多模态、多视图和多阶段的统一模态对齐；2) 采用软标签进行困难负样本挖掘并加强困难负样本对区分；3) 设计门控交叉注意力模块集成答案词汇作为先验知识。

Result: 在RAD-VQA、SLAKE、PathVQA和VQA-2019等广泛使用的Med-VQA数据集上优于先前最先进方法。

Conclusion: 该框架通过统一的模态对齐、有效的困难负样本处理和精确的知识融合，显著提升了医学视觉问答的性能。

Abstract: Medical Visual Question Answering (Med-VQA) is a challenging task that
requires a deep understanding of both medical images and textual questions.
Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP)
have shown strong performance on the Med-VQA task, there is still no unified
solution for modality alignment, and the issue of hard negatives remains
under-explored. Additionally, commonly used knowledge fusion techniques for
Med-VQA may introduce irrelevant information. In this work, we propose a
framework to address these challenges through three key contributions: (1) a
unified solution for heterogeneous modality alignments across multiple levels,
modalities, views, and stages, leveraging methods like contrastive learning and
optimal transport theory; (2) a hard negative mining method that employs soft
labels for multi-modality alignments and enforces the hard negative pair
discrimination; and (3) a Gated Cross-Attention Module for Med-VQA that
integrates the answer vocabulary as prior knowledge and selects relevant
information from it. Our framework outperforms the previous state-of-the-art on
widely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation](https://arxiv.org/abs/2510.09011)
*Yincen Qu,Huan Xiao,Feng Li,Hui Zhou,Xiangying Dai*

Main category: cs.AI

TL;DR: 提出了一个统一的旅行规划基准，通过单一奖励整合细粒度标准，支持强化学习，并在大规模数据集上验证了RL方法在可行性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估旅行计划的可行性、可靠性和吸引力方面存在不足，需要更全面的评估框架来比较不同LLM的规划能力。

Method: 构建包含4870个查询的大规模数据集，开发统一的奖励评估器，采用测试时计算、神经符号方法、监督微调和GRPO强化学习等多种方法进行实验。

Result: 评估器与旅游专家标注达到60.75%的一致性，优于多个LLM作为评判基准。RL方法在行程可行性方面优于仅提示和监督基线，获得更高的统一奖励分数。

Conclusion: 提出的基准能够有效评估旅行规划质量，RL方法在提高规划可行性方面表现出色，为LLM的旅行规划能力提供了标准化评估框架。

Abstract: Travel planning is a valuable yet complex task that poses significant
challenges even for advanced large language models (LLMs). While recent
benchmarks have advanced in evaluating LLMs' planning capabilities, they often
fall short in evaluating feasibility, reliability, and engagement of travel
plans. We introduce a comprehensive benchmark for travel planning that unifies
fine-grained criteria into a single reward, enabling direct comparison of plan
quality and seamless integration with reinforcement learning (RL). Our
evaluator achieves moderate agreement with travel-expert annotations (60.75\%)
and outperforms multiple LLM-as-judge baselines. We further release a
large-scale dataset of 4,870 queries including 219 real-world, free-form
requests for generalization to authentic user intent. Using this benchmark, we
conduct extensive experiments across diverse methods and LLMs, including
test-time computation, neuro-symbolic approaches, supervised fine-tuning, and
RL via GRPO. Across base models, RL generally improves itinerary feasibility
over prompt-only and supervised baselines, yielding higher unified reward
scores.

</details>
