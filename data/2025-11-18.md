<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/abs/2511.11552)
*Dawei Zhu,Rui Meng,Jiefeng Chen,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CV

TL;DR: DocLens是一个工具增强的多智能体框架，通过"放大"证据来解决长视觉文档理解中的证据定位问题，在MMLongBench-Doc和FinRAGBench-V上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视觉文档理解中存在证据定位的根本挑战，难以检索相关页面并忽略视觉元素中的细粒度细节，导致性能有限和模型幻觉。

Method: 提出DocLens框架，首先从完整文档导航到相关页面上的特定视觉元素，然后采用采样-裁决机制生成单一可靠答案。

Result: 与Gemini-2.5-Pro配对，DocLens在MMLongBench-Doc和FinRAGBench-V上达到最先进性能，甚至超越人类专家，在视觉中心和不可回答查询上表现尤为突出。

Conclusion: DocLens通过增强的定位能力展示了其优势，特别是在视觉中心和不可回答查询上，证明了其证据定位能力的强大效果。

Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.

</details>


### [2] [LARM: A Large Articulated-Object Reconstruction Model](https://arxiv.org/abs/2511.11563)
*Sylvia Yuan,Ruoxi Shi,Xinyue Wei,Xiaoshuai Zhang,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: LARM是一个统一的feedforward框架，能够从稀疏视角图像重建3D关节物体，同时恢复详细几何、真实纹理和准确关节结构，无需密集监督即可实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法需要密集多视角输入和昂贵的逐实例优化，限制了可扩展性；而前馈方法通常产生粗糙几何、缺乏纹理重建，且依赖复杂多阶段流程。

Method: LARM将LVSM扩展到关节设置，使用基于transformer的架构联合推理相机姿态和关节变化，生成深度图和部件掩码以支持显式3D网格提取和关节估计。

Result: 实验表明LARM在新视角合成、状态合成和3D关节物体重建方面优于最先进方法，生成与输入图像紧密贴合的高质量网格。

Conclusion: LARM提供了一个可扩展且准确的框架，能够从稀疏视角实现高质量的3D关节物体重建，消除了对密集监督的需求。

Abstract: Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 本文提出了AI伦理熵的第二定律，证明未经约束的人工智能会自发偏离目标，需要持续的对齐工作来维持稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在训练后容易偏离原始目标，存在规范博弈和探索噪声导致的伦理熵增问题，需要建立量化框架来理解和管理AI对齐的稳定性。

Method: 定义伦理熵S = -Σ p(g_i; theta) ln p(g_i; theta)，证明其时间导数dS/dt ≥ 0，推导出临界对齐工作边界gamma_crit = (lambda_max / 2) ln N，并通过70亿参数模型的仿真验证理论。

Result: 未经对齐的70亿参数模型(N=7×10^9)伦理熵从0.32增加到1.69±1.08纳特，而使用gamma=20.4(1.5倍临界值)对齐工作的系统保持稳定在0.00±0.00纳特(p=4.19×10^-17)。

Conclusion: 该框架将AI对齐重新定义为连续热力学控制问题，为维护先进自主系统的稳定性和安全性提供了量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>
